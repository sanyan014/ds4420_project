{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89da00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Manual MLP Components\n",
    "# -----------------------------\n",
    "\n",
    "# Activation Functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    # y_true must be one-hot encoded\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-9)\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10caa5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# MLP Initialization\n",
    "# -----------------------------\n",
    "def init_mlp(input_dim, hidden_dim, output_dim):\n",
    "    params = {\n",
    "        \"W1\": np.random.randn(input_dim, hidden_dim) * 0.01,\n",
    "        \"b1\": np.zeros((1, hidden_dim)),\n",
    "        \"W2\": np.random.randn(hidden_dim, output_dim) * 0.01,\n",
    "        \"b2\": np.zeros((1, output_dim))\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# -----------------------------\n",
    "# Forward Pass\n",
    "# -----------------------------\n",
    "def forward_pass(X, params):\n",
    "    Z1 = np.dot(X, params[\"W1\"]) + params[\"b1\"]\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, params[\"W2\"]) + params[\"b2\"]\n",
    "    A2 = softmax(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "# -----------------------------\n",
    "# Backward Pass (Backprop)\n",
    "# -----------------------------\n",
    "def backward_pass(X, y_true, params, cache):\n",
    "    m = X.shape[0]\n",
    "    dZ2 = cache[\"A2\"] - y_true\n",
    "    dW2 = np.dot(cache[\"A1\"].T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, params[\"W2\"].T)\n",
    "    dZ1 = dA1 * relu_derivative(cache[\"Z1\"])\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "# -----------------------------\n",
    "# Update Parameters\n",
    "# -----------------------------\n",
    "def update_params(params, grads, lr):\n",
    "    params[\"W1\"] -= lr * grads[\"dW1\"]\n",
    "    params[\"b1\"] -= lr * grads[\"db1\"]\n",
    "    params[\"W2\"] -= lr * grads[\"dW2\"]\n",
    "    params[\"b2\"] -= lr * grads[\"db2\"]\n",
    "    return params\n",
    "\n",
    "# -----------------------------\n",
    "# One Training Step (Batch)\n",
    "# -----------------------------\n",
    "def train_step(X, y_true, params, lr=0.01):\n",
    "    y_pred, cache = forward_pass(X, params)\n",
    "    loss = cross_entropy_loss(y_pred, y_true)\n",
    "    grads = backward_pass(X, y_true, params, cache)\n",
    "    params = update_params(params, grads, lr)\n",
    "    return loss, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Simulate Training with Dummy Data\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "n_samples = 10\n",
    "n_features = 6\n",
    "n_classes = 3\n",
    "\n",
    "# Fake dataset (10 samples, 6 features)\n",
    "X_dummy = np.random.rand(n_samples, n_features)\n",
    "# Fake labels (one-hot)\n",
    "y_dummy_labels = np.random.randint(0, n_classes, size=n_samples)\n",
    "y_dummy = np.eye(n_classes)[y_dummy_labels]\n",
    "\n",
    "# Initialize MLP\n",
    "params = init_mlp(input_dim=n_features, hidden_dim=8, output_dim=n_classes)\n",
    "\n",
    "# Train for 10 epochs\n",
    "for epoch in range(10):\n",
    "    loss, params = train_step(X_dummy, y_dummy, params, lr=0.1)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
