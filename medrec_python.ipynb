{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b0698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "    n   TP  FP   FN     TN  precision    recall        F1  FPR       TPR\n",
      "0   1   67   0  587  39010        1.0  0.102446  0.185853  0.0  0.102446\n",
      "1   3  195   0  459  39010        1.0  0.298165  0.459364  0.0  0.298165\n",
      "2   5  301   0  353  39010        1.0  0.460245  0.630366  0.0  0.460245\n",
      "3  10  482   0  172  39010        1.0  0.737003  0.848592  0.0  0.737003\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"final_flat_dataset.csv\")\n",
    "\n",
    "# Filter and deduplicate\n",
    "df_expanded = df[(df['drug'].notna()) & (df['drug'] != \"\") & (df['diagnosis'].notna())]\n",
    "df_expanded = df_expanded[['diagnosis', 'drug']].drop_duplicates()\n",
    "\n",
    "# Create binary matrix\n",
    "binary_df = df_expanded.copy()\n",
    "binary_df['value'] = 1\n",
    "binary_df = binary_df.pivot_table(index='diagnosis', columns='drug', values='value', fill_value=0)\n",
    "\n",
    "# Prepare data for surprise\n",
    "binary_long = binary_df.reset_index().melt(id_vars='diagnosis', var_name='drug', value_name='value')\n",
    "binary_long = binary_long[binary_long['value'] > 0]\n",
    "\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "data = Dataset.load_from_df(binary_long[['diagnosis', 'drug', 'value']], reader)\n",
    "\n",
    "# Train/test split\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# IBCF (item-based collaborative filtering)\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False  # Item-based\n",
    "}\n",
    "\n",
    "model = KNNBasic(sim_options=sim_options)\n",
    "model.fit(trainset)\n",
    "\n",
    "# Generate top-N predictions\n",
    "def get_top_n(predictions, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = [iid for (iid, _) in user_ratings[:n]]\n",
    "    return top_n\n",
    "\n",
    "predictions = model.test(testset)\n",
    "topn_preds = {n: get_top_n(predictions, n=n) for n in [1, 3, 5, 10]}\n",
    "\n",
    "# Evaluate: binary relevance metrics\n",
    "def calc_confusion(preds_dict, true_items_dict, all_items):\n",
    "    metrics = []\n",
    "    for n, preds in preds_dict.items():\n",
    "        TP = FP = FN = TN = 0\n",
    "        for uid, preds_n in preds.items():\n",
    "            true_items = true_items_dict.get(uid, [])\n",
    "            for item in all_items:\n",
    "                if item in preds_n and item in true_items:\n",
    "                    TP += 1\n",
    "                elif item in preds_n and item not in true_items:\n",
    "                    FP += 1\n",
    "                elif item not in preds_n and item in true_items:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "        precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) else 0\n",
    "        metrics.append({'n': n, 'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN,\n",
    "                        'precision': precision, 'recall': recall, 'F1': f1,\n",
    "                        'FPR': fpr, 'TPR': recall})\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Prepare ground truth for evaluation\n",
    "true_items_dict = defaultdict(list)\n",
    "for uid, iid, true_r in testset:\n",
    "    if true_r > 0:\n",
    "        true_items_dict[uid].append(iid)\n",
    "\n",
    "all_items = set(df_expanded['drug'])\n",
    "\n",
    "df_metrics = calc_confusion(topn_preds, true_items_dict, all_items)\n",
    "df_metrics.to_csv(\"ibcf_summary_metrics.csv\", index=False)\n",
    "\n",
    "print(df_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds)",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
